
# ======== Event Log ========

## for S3 (does not yet work)
#spark.history.provider         org.apache.spark.deploy.history.FsHistoryProvider#
#spark.eventLog.dir            s3a://spark-logs-abc123/logs
#spark.history.fs.logDirectory s3a://spark-logs-abc123/logs

## for HDFS
spark.history.provider        org.apache.spark.deploy.history.FsHistoryProvider
spark.eventLog.dir            hdfs://namenode:9000/var/log/spark/logs
spark.history.fs.logDirectory hdfs://namenode:9000/var/log/spark/logs

spark.eventLog.enabled true
spark.eventLog.overwrite true

# ======== S3 Access ========
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint s3.eu-central-1.amazonaws.com
#spark.hadoop.fs.s3a.path.style.access true

# specify AWS dependencies to access S3 as packages which are loaded via Maven
spark.jars.packages   com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3

#spark.driver.extraClassPath 	/hadoop-2.7.3/share/hadoop/tools/lib/*
#spark.executor.extraClassPath	/hadoop-2.7.3/share/hadoop/tools/lib/*

# Necessary for Frankfurt to work with S3
spark.driver.extraJavaOptions    -Dcom.amazonaws.services.s3.enableV4
spark.executor.extraJavaOptions  -Dcom.amazonaws.services.s3.enableV4

# ======== Hive Metastore ========
spark.sql.catalogImplementation=hive
spark.sql.warehouse.dir          hdfs://namenode:9000/user/hive/warehouse

# ======== YARN  ========

#  Comma-separated list of files to be placed in the working directory of each executor.
spark.yarn.dist.files            /etc/spark/conf/hive-site.xml


# ======== Not Used ========

#spark.yarn.historyServer.address=analyticsplatform:18080
